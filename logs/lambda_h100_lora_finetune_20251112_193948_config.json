{
  "model": "facebook/opt-1.3b",
  "batch_size": 16,
  "gradient_accumulation_steps": 2,
  "num_epochs": 3,
  "max_steps": 1000,
  "lora_r": 16,
  "lora_alpha": 32,
  "lora_dropout": 0.05,
  "use_4bit": true,
  "max_seq_length": 512,
  "platform": "lambda_h100"
}